# coding:utf-8
from models.network import NetWork
import tensorflow as tf
from component import *

def choose_generator(g_name, image_batch):
    if '16' in g_name:
        return Generator_vgg_16({'data': image_batch})
    elif '121' in g_name:
        return Generator_densenet121({'data': image_batch})
    elif '50' in g_name:
        return Generator_resnet50({'data': image_batch})
    elif '101' in g_name:
        return Generator_resnet101({'data': image_batch})


class Generator_vgg_16(NetWork):
    def setup(self, is_training, num_classes):
        name = 'generator/'
        (self.feed('data')
         .conv([3, 3], 64, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv1_1')
         .conv([3, 3], 64, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv1_2')
         .max_pool([2, 2], [2, 2], name=name + 'image_pool1')
         .conv([3, 3], 128, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv2_1')
         .conv([3, 3], 128, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv2_2')
         .max_pool([2, 2], [2, 2], name=name + 'image_pool2')
         .conv([3, 3], 256, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv3_1')
         .conv([3, 3], 256, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv3_2')
         .conv([3, 3], 256, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv3_3')
         .max_pool([2, 2], [2, 2], name=name + 'image_pool3')
         .conv([3, 3], 512, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv4_1')
         .conv([3, 3], 512, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv4_2')
         .conv([3, 3], 512, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv4_3')
         .max_pool([2, 2], [2, 2], name=name + 'image_pool4')
         .conv([3, 3], 512, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv5_1')
         .conv([3, 3], 512, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv5_2')
         .conv([3, 3], 512, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_conv5_3')
         .max_pool([2, 2], [2, 2], name=name + 'image_pool5')
         .conv([7, 7], 4096, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_fc6')
         .dropout(keep_prob=0.5, name=name + 'drop6')
         .conv([1, 1], 4096, [1, 1], reuse=self.reuse, biased=True, relu=True, name=name + 'image_fc7')
         .dropout(keep_prob=0.5, name=name + 'drop7')
         .conv([1, 1], num_classes, [1, 1], reuse=self.reuse, biased=True, relu=False, name=name + 'score_fr'))

        (self.feed(name + 'image_pool4')
         .conv([1, 1], num_classes, [1, 1], reuse=self.reuse, biased=True, relu=False, name=name + 'score_pool4'))
        pool_shape = tf.shape(self.layers[name + 'score_pool4'])

        (self.feed(name + 'score_fr')
         .deconv([4, 4], pool_shape, [2, 2], num_classes, reuse=self.reuse, biased=False, relu=False,
                 name=name + 'upscore2'))
        (self.feed(name + 'upscore2', name + 'score_pool4')
         .add(name=name + 'fuse_pool4'))
        origin_shape = tf.multiply(tf.shape(self.layers['data']), tf.convert_to_tensor([1, 1, 1, 7]))

        (self.feed(name + 'fuse_pool4')
         .deconv([32, 32], origin_shape, [16, 16], num_classes, reuse=self.reuse, biased=False, relu=False,
                 name=name + 'upscore16'))

    def topredict(self, raw_output, origin_shape=None):
        raw_output = tf.image.resize_bilinear(raw_output, origin_shape)
        raw_output = tf.argmax(raw_output, axis=3)
        prediction = tf.expand_dims(raw_output, dim=3)
        return prediction


class Generator_resnet50(NetWork):
    def setup(self, is_training, num_classes):
        (self.feed('data')
         .conv([7, 7], 64, [2, 2], biased=False, relu=False, name='conv1', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_conv1', activation=True)
         .max_pool([3, 3], [2, 2], name='pool1')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2a_branch1', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2a_branch1', activation=True))

        (self.feed('pool1')
         .conv([1, 1], 64, [1, 1], biased=False, relu=False, name='res2a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2a_branch2a')
         .conv([3, 3], 64, [1, 1], biased=False, relu=False, name='res2a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2a_branch2b')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2a_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2a_branch2c', activation=True))

        (self.feed('bn2a_branch1',
                   'bn2a_branch2c')
         .add(name='res2a')
         .relu(name='res2a_relu')
         .conv([1, 1], 64, [1, 1], biased=False, relu=False, name='res2b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2b_branch2a')
         .conv([3, 3], 64, [1, 1], biased=False, relu=False, name='res2b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2b_branch2b')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2b_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2b_branch2c', activation=True))

        (self.feed('res2a_relu',
                   'bn2b_branch2c')
         .add(name='res2b')
         .relu(name='res2b_relu')
         .conv([1, 1], 64, [1, 1], biased=False, relu=False, name='res2c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2c_branch2a')
         .conv([3, 3], 64, [1, 1], biased=False, relu=False, name='res2c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2c_branch2b')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2c_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2c_branch2c', activation=True))

        (self.feed('res2b_relu',
                   'bn2c_branch2c')
         .add(name='res2c')
         .relu(name='res2c_relu')
         .conv([1, 1], 512, [2, 2], biased=False, relu=False, name='res3a_branch1', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3a_branch1', activation=True))

        (self.feed('res2c_relu')
         .conv([1, 1], 128, [2, 2], biased=False, relu=False, name='res3a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3a_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3a_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3a_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3a_branch2c', activation=True))

        (self.feed('bn3a_branch1',
                   'bn3a_branch2c')
         .add(name='res3a')
         .relu(name='res3a_relu')
         .conv([1, 1], 128, [1, 1], biased=False, relu=False, name='res3b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3b_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3b_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3b_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3b_branch2c', activation=True))

        (self.feed('res3a_relu',
                   'bn3b_branch2c')
         .add(name='res3b')
         .relu(name='res3b_relu')
         .conv([1, 1], 128, [1, 1], biased=False, relu=False, name='res3c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3c_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3c_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3c_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3c_branch2c', activation=True))

        (self.feed('res3b_relu',
                   'bn3c_branch2c')
         .add(name='res3c')
         .relu(name='res3c_relu')
         .conv([1, 1], 128, [1, 1], biased=False, relu=False, name='res3d_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3d_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3d_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3d_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3d_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3d_branch2c', activation=True))

        (self.feed('res3c_relu',
                   'bn3d_branch2c')
         .add(name='res3d')
         .relu(name='res3d_relu')
         .conv([1, 1], 1024, [2, 2], biased=False, relu=False, name='res4a_branch1', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4a_branch1', activation=True))

        (self.feed('res3d_relu')
         .conv([1, 1], 256, [2, 2], biased=False, relu=False, name='res4a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4a_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4a_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4a_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4a_branch2c', activation=True))
        (self.feed('bn4a_branch1',
                   'bn4a_branch2c')
         .add(name='res4a')
         .relu(name='res4a_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4b_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4b_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4b_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4b_branch2c', activation=True))

        (self.feed('res4a_relu',
                   'bn4b_branch2c')
         .add(name='res4b')
         .relu(name='res4b_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4c_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4c_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4c_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4c_branch2c', activation=True))

        (self.feed('res4b_relu',
                   'bn4c_branch2c')
         .add(name='res4c')
         .relu(name='res4c_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4d_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4d_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4d_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4d_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4d_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4d_branch2c', activation=True))

        (self.feed('res4c_relu',
                   'bn4d_branch2c')
         .add(name='res4d')
         .relu(name='res4d_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4e_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4e_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4e_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4e_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4e_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4e_branch2c', activation=True))

        (self.feed('res4d_relu',
                   'bn4e_branch2c')
         .add(name='res4e')
         .relu(name='res4e_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4f_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4f_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4f_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4f_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4f_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4f_branch2c', activation=True))

        (self.feed('res4e_relu',
                   'bn4f_branch2c')
         .add(name='res4f')
         .relu(name='res4f_relu')
         .conv([1, 1], 2048, [2, 2], biased=False, relu=False, name='res5a_branch1', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5a_branch1', activation=True))

        (self.feed('res4e_relu')
         .conv([1, 1], 512, [2, 2], biased=False, relu=False, name='res5a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5a_branch2a')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name='res5a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5a_branch2b')
         .conv([1, 1], 2048, [1, 1], biased=False, relu=False, name='res5a_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5a_branch2c', activation=True))

        (self.feed('bn5a_branch1',
                   'bn5a_branch2c')
         .add(name='res5a')
         .relu(name='res5a_relu')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res5b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5b_branch2a')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name='res5b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5b_branch2b')
         .conv([1, 1], 2048, [1, 1], biased=False, relu=False, name='res5b_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5b_branch2c', activation=True))

        (self.feed('res5a_relu',
                   'bn5b_branch2c')
         .add(name='res5b')
         .relu(name='res5b_relu')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res5c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5c_branch2a')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name='res5c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5c_branch2b')
         .conv([1, 1], 2048, [1, 1], biased=False, relu=False, name='res5c_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5c_branch2c', activation=True))

        (self.feed('res5b_relu',
                   'bn5c_branch2c')
         .add(name='res5c')
         .relu(name='res5c_relu')
         .atrous_conv([3, 3], num_classes, 6, padding='SAME', relu=False, name='fc1_voc12_c0'))

        (self.feed('res5c_relu')
         .atrous_conv([3, 3], num_classes, 12, padding='SAME', relu=False, name='fc1_voc12_c1'))

        (self.feed('res5c_relu')
         .atrous_conv([3, 3], num_classes, 18, padding='SAME', relu=False, name='fc1_voc12_c2'))

        (self.feed('res5c_relu')
         .atrous_conv([3, 3], num_classes, 24, padding='SAME', relu=False, name='fc1_voc12_c3'))

        (self.feed('res5c_relu')
         .global_average_pooling(name='global_pool'))
        now_name_5 = 'cab5'
        (self.feed('res5c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_5 + '_bn',
                              activation=False)
         .leaky_relu(name=now_name_5 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv3'))
        (self.feed(now_name_5 + '_conv1',
                   now_name_5 + '_conv3')
         .add(name=now_name_5)
         .leaky_relu(name=now_name_5 + '_relu'))
        cab5_shape = tf.shape(self.layers[now_name_5])
        layer = self.get_appointed_layer(now_name_5 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('global_pool')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_5 + '_upscore', padding='VALID')
         .resize(new_shape, name=now_name_5 + '_upscore'))
        (self.feed(now_name_5 + '_upscore', now_name_5 + '_relu')
         .concat(axis=3, name=now_name_5 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_5 + '_avgpool')  # too many time
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv4')
         .leaky_relu(name=now_name_5 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv5')
         .sigmoid(name=now_name_5 + '_sigmoid')
         .multiply(now_name_5 + '_relu', name=now_name_5 + '_mul'))
        (self.feed(now_name_5 + '_mul', now_name_5 + '_upscore')
         .add(name=now_name_5 + '_output'))
        (self.feed(now_name_5 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_5 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_5 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv8'))
        (self.feed(now_name_5 + '_conv6',
                   now_name_5 + '_conv8')
         .add(name=now_name_5 + '_last')
         .leaky_relu(name=now_name_5 + '_relu_last'))
        now_name_4 = 'cab4'
        (self.feed('res4c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv3'))
        (self.feed(now_name_4 + '_conv1',
                   now_name_4 + '_conv3')
         .add(name=now_name_4))
        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab5' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))
        factor = 16
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))

        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + 'b4', padding='SAME')
         .resize(new_shape, name=now_name_4 + 'b4'))
        (self.feed(now_name_4 + '_upscore', now_name_4 + '_relu')
         .concat(axis=3, name=now_name_4 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_4 + '_avgpool')  # too many time
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4_1')
         .leaky_relu(name=now_name_4 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv5')
         .sigmoid(name=now_name_4 + '_sigmoid')
         .multiply(now_name_4 + '_relu', name=now_name_4 + '_mul'))
        (self.feed(now_name_4 + '_mul', now_name_4 + '_upscore')
         .add(name=now_name_4 + '_output'))
        (self.feed(now_name_4 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv8'))
        (self.feed(now_name_4 + '_conv6',
                   now_name_4 + '_conv8')
         .add(name=now_name_4 + '_last')
         .leaky_relu(name=now_name_4 + '_relu_last'))
        now_name_4 = 'cab2'
        (self.feed('res2c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv3'))
        (self.feed(now_name_4 + '_conv1',
                   now_name_4 + '_conv3')
         .add(name=now_name_4)
         .leaky_relu(name=now_name_4 + '_relu'))
        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab4' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))
        factor = 4
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))

        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + 'b2', padding='SAME')
         .resize(new_shape, name=now_name_4 + 'b2'))
        (self.feed(now_name_4 + '_upscore', now_name_4 + '_relu')
         .concat(axis=3, name=now_name_4 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_4 + '_avgpool')  # too many time
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4_1')
         .leaky_relu(name=now_name_4 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv5')
         .sigmoid(name=now_name_4 + '_sigmoid')
         .multiply(now_name_4 + '_relu', name=now_name_4 + '_mul'))
        (self.feed(now_name_4 + '_mul', now_name_4 + '_upscore')
         .add(name=now_name_4 + '_output'))
        (self.feed(now_name_4 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv8'))
        (self.feed(now_name_4 + '_conv6',
                   now_name_4 + '_conv8')
         .add(name=now_name_4 + '_last')
         .leaky_relu(name=now_name_4 + '_relu_last'))
        now_name_4 = 'cab1'
        cab5_shape = tf.shape(self.layers['cab2' + '_relu_last'])
        layer = self.get_appointed_layer('cab2' + '_relu_last')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab2' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))
        factor = 2
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))
        cab5_shape = tf.shape(self.layers[now_name_4 + '_conv4'])
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=True, relu=False,
                 name=now_name_4 + 'b1', padding='SAME'))
        (self.feed('cab1b1', 'cab2b2', 'cab4b4')
         .concat(axis=3, name='All_Bias')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name='fuse'))
        (self.feed('fc1_voc12_c0',
                   'fc1_voc12_c1',
                   'fc1_voc12_c2',
                   'fc1_voc12_c3')
         .add(name='fc1_voc12'))

    def topredict(self, raw_output, origin_shape=None):
        raw_output = tf.image.resize_bilinear(raw_output, origin_shape)
        raw_output = tf.argmax(raw_output, axis=3)
        prediction = tf.expand_dims(raw_output, dim=3)
        return prediction


class Generator_resnet101(NetWork):
    def setup(self, is_training, num_classes):
        (self.feed('data')
         .conv([7, 7], 64, [2, 2], biased=False, relu=False, name='conv1', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_conv1', activation=False)
         .leaky_relu(name='conv1_relu', alpha=0.1)
         .max_pool([3, 3], [2, 2], name='pool1')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2a_branch1', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2a_branch1', activation=False))

        (self.feed('pool1')
         .conv([1, 1], 64, [1, 1], biased=False, relu=False, name='res2a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2a_branch2a')
         .conv([3, 3], 64, [1, 1], biased=False, relu=False, name='res2a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2a_branch2b')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2a_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2a_branch2c', activation=False))

        (self.feed('bn2a_branch1',
                   'bn2a_branch2c')
         .add(name='res2a')
         .relu(name='res2a_relu')
         .conv([1, 1], 64, [1, 1], biased=False, relu=False, name='res2b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2b_branch2a')
         .conv([3, 3], 64, [1, 1], biased=False, relu=False, name='res2b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2b_branch2b')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2b_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2b_branch2c', activation=False))

        (self.feed('res2a_relu',
                   'bn2b_branch2c')
         .add(name='res2b')
         .relu(name='res2b_relu')
         .conv([1, 1], 64, [1, 1], biased=False, relu=False, name='res2c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2c_branch2a')
         .conv([3, 3], 64, [1, 1], biased=False, relu=False, name='res2c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn2c_branch2b')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res2c_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn2c_branch2c', activation=False))

        (self.feed('res2b_relu',
                   'bn2c_branch2c')
         .add(name='res2c')
         .relu(name='res2c_relu')
         .conv([1, 1], 512, [2, 2], biased=False, relu=False, name='res3a_branch1', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3a_branch1', activation=False))

        (self.feed('res2c_relu')
         .conv([1, 1], 128, [2, 2], biased=False, relu=False, name='res3a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3a_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3a_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3a_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3a_branch2c', activation=False))

        (self.feed('bn3a_branch1',
                   'bn3a_branch2c')
         .add(name='res3a')
         .relu(name='res3a_relu')
         .conv([1, 1], 128, [1, 1], biased=False, relu=False, name='res3b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3b_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3b_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3b_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3b_branch2c', activation=False))

        (self.feed('res3a_relu',
                   'bn3b_branch2c')
         .add(name='res3b')
         .relu(name='res3b_relu')
         .conv([1, 1], 128, [1, 1], biased=False, relu=False, name='res3c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3c_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3c_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3c_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3c_branch2c', activation=False))

        (self.feed('res3b_relu',
                   'bn3c_branch2c')
         .add(name='res3c')
         .relu(name='res3c_relu')
         .conv([1, 1], 128, [1, 1], biased=False, relu=False, name='res3d_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3d_branch2a')
         .conv([3, 3], 128, [1, 1], biased=False, relu=False, name='res3d_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn3d_branch2b')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res3d_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn3d_branch2c', activation=False))

        (self.feed('res3c_relu',
                   'bn3d_branch2c')
         .add(name='res3d')
         .relu(name='res3d_relu')
         .conv([1, 1], 1024, [2, 2], biased=False, relu=False, name='res4a_branch1', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4a_branch1', activation=False))

        (self.feed('res3d_relu')
         .conv([1, 1], 256, [2, 2], biased=False, relu=False, name='res4a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4a_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4a_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4a_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4a_branch2c', activation=False))

        (self.feed('bn4a_branch1',
                   'bn4a_branch2c')
         .add(name='res4a')
         .relu(name='res4a_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4b_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4b_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4b_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4b_branch2c', activation=False))

        (self.feed('res4a_relu',
                   'bn4b_branch2c')
         .add(name='res4b')
         .relu(name='res4b_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4c_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4c_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4c_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4c_branch2c', activation=False))

        (self.feed('res4b_relu',
                   'bn4c_branch2c')
         .add(name='res4c')
         .relu(name='res4c_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4d_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4d_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4d_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4d_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4d_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4d_branch2c', activation=False))

        (self.feed('res4c_relu',
                   'bn4d_branch2c')
         .add(name='res4d')
         .relu(name='res4d_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4e_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4e_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4e_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4e_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4e_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4e_branch2c', activation=False))

        (self.feed('res4d_relu',
                   'bn4e_branch2c')
         .add(name='res4e')
         .relu(name='res4e_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4f_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4f_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4f_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4f_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4f_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4f_branch2c', activation=False))

        (self.feed('res4e_relu',
                   'bn4f_branch2c')
         .add(name='res4f')
         .relu(name='res4f_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4g_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4g_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4g_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4g_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4g_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4g_branch2c', activation=False))

        (self.feed('res4f_relu',
                   'bn4g_branch2c')
         .add(name='res4g')
         .relu(name='res4g_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4h_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4h_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4h_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4h_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4h_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4h_branch2c', activation=False))

        (self.feed('res4g_relu',
                   'bn4h_branch2c')
         .add(name='res4h')
         .relu(name='res4h_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4i_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4i_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4i_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4i_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4i_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4i_branch2c', activation=False))

        (self.feed('res4h_relu',
                   'bn4i_branch2c')
         .add(name='res4i')
         .relu(name='res4i_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4j_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4j_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4j_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4j_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4j_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4j_branch2c', activation=False))

        (self.feed('res4i_relu',
                   'bn4j_branch2c')
         .add(name='res4j')
         .relu(name='res4j_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4k_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4k_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4k_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4k_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4k_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4k_branch2c', activation=False))

        (self.feed('res4j_relu',
                   'bn4k_branch2c')
         .add(name='res4k')
         .relu(name='res4k_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4l_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4l_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4l_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4l_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4l_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4l_branch2c', activation=False))

        (self.feed('res4k_relu',
                   'bn4l_branch2c')
         .add(name='res4l')
         .relu(name='res4l_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4m_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4m_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4m_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4m_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4m_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4m_branch2c', activation=False))

        (self.feed('res4l_relu',
                   'bn4m_branch2c')
         .add(name='res4m')
         .relu(name='res4m_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4n_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4n_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4n_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4n_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4n_branch2c', activation=True)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4n_branch2c', activation=True))

        (self.feed('res4m_relu',
                   'bn4n_branch2c')
         .add(name='res4n')
         .relu(name='res4n_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4o_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4o_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4o_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4o_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4o_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4o_branch2c', activation=False))

        (self.feed('res4n_relu',
                   'bn4o_branch2c')
         .add(name='res4o')
         .relu(name='res4o_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4p_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4p_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4p_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4p_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4p_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4p_branch2c', activation=False))

        (self.feed('res4o_relu',
                   'bn4p_branch2c')
         .add(name='res4p')
         .relu(name='res4p_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4q_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4q_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4q_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4q_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4q_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4q_branch2c', activation=False))

        (self.feed('res4p_relu',
                   'bn4q_branch2c')
         .add(name='res4q')
         .relu(name='res4q_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4r_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4r_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4r_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4r_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4r_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4r_branch2c', activation=False))

        (self.feed('res4q_relu',
                   'bn4r_branch2c')
         .add(name='res4r')
         .relu(name='res4r_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4s_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4s_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4s_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4s_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4s_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4s_branch2c', activation=False))

        (self.feed('res4r_relu',
                   'bn4s_branch2c')
         .add(name='res4s')
         .relu(name='res4s_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4t_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4t_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4t_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4t_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4t_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4t_branch2c', activation=False))

        (self.feed('res4s_relu',
                   'bn4t_branch2c')
         .add(name='res4t')
         .relu(name='res4t_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4u_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4u_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4u_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4u_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4u_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4u_branch2c', activation=False))

        (self.feed('res4t_relu',
                   'bn4u_branch2c')
         .add(name='res4u')
         .relu(name='res4u_relu')
         .conv([1, 1], 256, [1, 1], biased=False, relu=False, name='res4v_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4v_branch2a')
         .conv([3, 3], 256, [1, 1], padding='SAME', biased=False, relu=False, name='res4v_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn4v_branch2b')
         .conv([1, 1], 1024, [1, 1], biased=False, relu=False, name='res4v_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn4v_branch2c', activation=False))

        (self.feed('res4u_relu',
                   'bn4v_branch2c')
         .add(name='res4v')
         .relu(name='res4v_relu')
         .conv([1, 1], 2048, [2, 2], biased=False, relu=False, name='res5a_branch1', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5a_branch1', activation=False))

        (self.feed('res4e_relu')
         .conv([1, 1], 512, [2, 2], biased=False, relu=False, name='res5a_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5a_branch2a')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name='res5a_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5a_branch2b')
         .conv([1, 1], 2048, [1, 1], biased=False, relu=False, name='res5a_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5a_branch2c', activation=False))

        (self.feed('bn5a_branch1',
                   'bn5a_branch2c')
         .add(name='res5a')
         .relu(name='res5a_relu')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res5b_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5b_branch2a')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name='res5b_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5b_branch2b')
         .conv([1, 1], 2048, [1, 1], biased=False, relu=False, name='res5b_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5b_branch2c', activation=False))

        (self.feed('res5a_relu',
                   'bn5b_branch2c')
         .add(name='res5b')
         .relu(name='res5b_relu')
         .conv([1, 1], 512, [1, 1], biased=False, relu=False, name='res5c_branch2a')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5c_branch2a')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name='res5c_branch2b')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn5c_branch2b')
         .conv([1, 1], 2048, [1, 1], biased=False, relu=False, name='res5c_branch2c', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=None, name='bn5c_branch2c', activation=False))

        (self.feed('res5b_relu',
                   'bn5c_branch2c')
         .add(name='res5c')
         .relu(name='res5c_relu')
         .atrous_conv([3, 3], num_classes, 6, padding='SAME', relu=False, name='fc1_voc12_c0'))

        (self.feed('res5c_relu')
         .atrous_conv([3, 3], num_classes, 12, padding='SAME', relu=False, name='fc1_voc12_c1'))

        (self.feed('res5c_relu')
         .atrous_conv([3, 3], num_classes, 18, padding='SAME', relu=False, name='fc1_voc12_c2'))

        (self.feed('res5c_relu')
         .atrous_conv([3, 3], num_classes, 24, padding='SAME', relu=False, name='fc1_voc12_c3'))

        (self.feed('res5c_relu')
         .global_average_pooling(name='global_pool'))
        now_name_5 = 'cab5'
        (self.feed('res5c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_5 + '_bn',
                              activation=False)
         .leaky_relu(name=now_name_5 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv3'))
        (self.feed(now_name_5 + '_conv1',
                   now_name_5 + '_conv3')
         .add(name=now_name_5)
         .leaky_relu(name=now_name_5 + '_relu'))
        cab5_shape = tf.shape(self.layers[now_name_5])
        layer = self.get_appointed_layer(now_name_5 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('global_pool')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_5 + '_upscore', padding='VALID')
         .resize(new_shape, name=now_name_5 + '_upscore'))
        (self.feed(now_name_5 + '_upscore', now_name_5 + '_relu')
         .concat(axis=3, name=now_name_5 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_5 + '_avgpool')  # too many time
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv4')
         .leaky_relu(name=now_name_5 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv5')
         .sigmoid(name=now_name_5 + '_sigmoid')
         .multiply(now_name_5 + '_relu', name=now_name_5 + '_mul'))
        (self.feed(now_name_5 + '_mul', now_name_5 + '_upscore')
         .add(name=now_name_5 + '_output'))
        (self.feed(now_name_5 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_5 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_5 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_5 + '_conv8'))
        (self.feed(now_name_5 + '_conv6',
                   now_name_5 + '_conv8')
         .add(name=now_name_5 + '_last')
         .leaky_relu(name=now_name_5 + '_relu_last'))
        now_name_4 = 'cab4'
        (self.feed('res4c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv3'))
        (self.feed(now_name_4 + '_conv1',
                   now_name_4 + '_conv3')
         .add(name=now_name_4)
         .leaky_relu(name=now_name_4 + '_relu'))
        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab5' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))
        factor = 16
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))

        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + 'b4', padding='SAME')
         .resize(new_shape, name=now_name_4 + 'b4'))
        (self.feed(now_name_4 + '_upscore', now_name_4 + '_relu')
         .concat(axis=3, name=now_name_4 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_4 + '_avgpool')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4_1')
         .leaky_relu(name=now_name_4 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv5')
         .sigmoid(name=now_name_4 + '_sigmoid')
         .multiply(now_name_4 + '_relu', name=now_name_4 + '_mul'))
        (self.feed(now_name_4 + '_mul', now_name_4 + '_upscore')
         .add(name=now_name_4 + '_output'))
        (self.feed(now_name_4 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv8'))
        (self.feed(now_name_4 + '_conv6',
                   now_name_4 + '_conv8')
         .add(name=now_name_4 + '_last')
         .leaky_relu(name=now_name_4 + '_relu_last'))
        now_name_4 = 'cab3'
        (self.feed('res3c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn',
                              activation=True)
         .leaky_relu(name=now_name_4 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv3'))
        (self.feed(now_name_4 + '_conv1',
                   now_name_4 + '_conv3')
         .add(name=now_name_4)
         .leaky_relu(name=now_name_4 + '_relu'))
        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab4' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))
        factor = 8
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))

        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + 'b3', padding='SAME')
         .resize(new_shape, name=now_name_4 + 'b3'))
        # op1 = self.get_appointed_layer(layer1)
        (self.feed(now_name_4 + '_upscore', now_name_4 + '_relu')
         .concat(axis=3, name=now_name_4 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_4 + '_avgpool')  # too many time
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4_1')
         .leaky_relu(name=now_name_4 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv5')
         .sigmoid(name=now_name_4 + '_sigmoid')
         .multiply(now_name_4 + '_relu', name=now_name_4 + '_mul'))
        (self.feed(now_name_4 + '_mul', now_name_4 + '_upscore')
         .add(name=now_name_4 + '_output'))
        (self.feed(now_name_4 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv8'))
        (self.feed(now_name_4 + '_conv6',
                   now_name_4 + '_conv8')
         .add(name=now_name_4 + '_last')
         .leaky_relu(name=now_name_4 + '_relu_last'))
        now_name_4 = 'cab2'
        (self.feed('res2c_relu')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu1')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv3'))
        (self.feed(now_name_4 + '_conv1',
                   now_name_4 + '_conv3')
         .add(name=now_name_4)
         .leaky_relu(name=now_name_4 + '_relu'))
        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab3' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))
        factor = 4
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))

        cab5_shape = tf.shape(self.layers[now_name_4])
        layer = self.get_appointed_layer(now_name_4 + '_relu')
        new_shape = tf.shape(layer)[1:3]
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + 'b2', padding='SAME')
         .resize(new_shape, name=now_name_4 + 'b2'))
        (self.feed(now_name_4 + '_upscore', now_name_4 + '_relu')
         .concat(axis=3, name=now_name_4 + '_input')
         .avg_pool(kernel=[16, 16], strides=[16, 16], name=now_name_4 + '_avgpool')  # too many time
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4_1')
         .leaky_relu(name=now_name_4 + '_relu2')
         .conv([1, 1], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv5')
         .sigmoid(name=now_name_4 + '_sigmoid')
         .multiply(now_name_4 + '_relu', name=now_name_4 + '_mul'))
        (self.feed(now_name_4 + '_mul', now_name_4 + '_upscore')
         .add(name=now_name_4 + '_output'))
        (self.feed(now_name_4 + '_output')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv6')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv7')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name=now_name_4 + '_bn1',
                              activation=False)
         .leaky_relu(name=now_name_4 + '_relu3')
         .conv([3, 3], 512, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv8'))
        (self.feed(now_name_4 + '_conv6',
                   now_name_4 + '_conv8')
         .add(name=now_name_4 + '_last')
         .leaky_relu(name=now_name_4 + '_relu_last'))
        now_name_4 = 'cab1'
        cab5_shape = tf.shape(self.layers['cab2' + '_relu_last'])
        layer = self.get_appointed_layer('cab2' + '_relu_last')
        new_shape = tf.shape(layer)[1:3]
        (self.feed('cab2' + '_relu_last')
         .deconv([16, 16], cab5_shape, [2, 2], 512, reuse=self.reuse, biased=False, relu=False,
                 name=now_name_4 + '_upscore', padding='SAME')
         .resize(new_shape, name=now_name_4 + '_upscore'))

        # Side-Branch
        factor = 2
        (self.feed(now_name_4 + '_upscore')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name=now_name_4 + '_conv4'))

        cab5_shape = tf.shape(self.layers[now_name_4 + '_conv4'])
        (self.feed(now_name_4 + '_conv4')
         .deconv([factor * 2, factor * 2], cab5_shape, [factor, factor], num_classes,
                 reuse=self.reuse, biased=True, relu=False,
                 name=now_name_4 + 'b1', padding='SAME'))

        (self.feed('cab1b1', 'cab2b2', 'cab3b3', 'cab4b4')
         .concat(axis=3, name='All_Bias')
         .conv([1, 1], num_classes, [1, 1], padding='SAME', biased=False, relu=False, name='fuse'))



        (self.feed('fc1_voc12_c0',
                   'fc1_voc12_c1',
                   'fc1_voc12_c2',
                   'fc1_voc12_c3')
         .add(name='fc1_voc12'))

    def topredict(self, raw_output, origin_shape=None):
        raw_output = tf.image.resize_bilinear(raw_output, origin_shape)
        raw_output = tf.argmax(raw_output, axis=3)
        prediction = tf.expand_dims(raw_output, dim=3)
        return prediction


class Generator_densenet121(NetWork):
    def setup(self, is_training, num_classes):
        num_filters = 64
        growth_rate = 32
        compresson = 0.5
        (self.feed('data')
         .conv([7, 7], 64, [2, 2], biased=False, relu=False, name='conv1', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_conv1', activation=False)
         .relu(name='conv1_relu')
         .max_pool([3, 3], [2, 2], name='pool1'))
        (self.feed('pool1')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block0_conv1',
                              activation=False)
         .relu(name='block1_conv0_relu')
         .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block1_conv0_conv1', activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block0_conv2',
                              activation=False)
         .relu(name='block1_conv0_relu1')
         .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block1_conv0_conv2', activation=False))
        (self.feed('block1_conv0_conv2', 'pool1')
         .concat(axis=3, name='block1_conv0'))
        num_filters += growth_rate

        (self.feed('block1_conv0')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv1_conv1',
                              activation=False)
         .relu(name='block1_conv1_relu')
         .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block1_conv1_conv1',
               activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv1_conv2',
                              activation=False)
         .relu(name=('block1_conv1_relu1'))
         .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block1_conv1_conv2', activation=False))
        (self.feed('block1_conv1_conv2', 'block1_conv0')
         .concat(axis=3, name='block1_conv1'))
        num_filters += growth_rate

        (self.feed('block1_conv1')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv2_conv1',
                              activation=False)
         .relu(name='block1_conv2_relu')
         .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block1_conv2_conv1',
               activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv2_conv2',
                              activation=False)
         .relu(name=('block1_conv2_relu1'))
         .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block1_conv2_conv2', activation=False))
        (self.feed('block1_conv2_conv2', 'block1_conv1')
         .concat(axis=3, name='block1_conv2'))
        num_filters += growth_rate

        (self.feed('block1_conv2')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv3_conv1',
                              activation=False)
         .relu(name='block1_conv3_relu')
         .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block1_conv3_conv1',
               activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv3_conv2',
                              activation=False)
         .relu(name=('block1_conv3_relu1'))
         .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block1_conv3_conv2', activation=False))
        (self.feed('block1_conv3_conv2', 'block1_conv2')
         .concat(axis=3, name='block1_conv3'))
        num_filters += growth_rate

        (self.feed('block1_conv3')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv4_conv1',
                              activation=False)
         .relu(name='block1_conv4_relu')
         .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block1_conv4_conv1',
               activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv4_conv2',
                              activation=False)
         .relu(name=('block1_conv4_relu1'))
         .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block1_conv4_conv2', activation=False))
        (self.feed('block1_conv4_conv2', 'block1_conv3')
         .concat(axis=3, name='block1_conv4'))
        num_filters += growth_rate

        (self.feed('block1_conv4')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv5_conv1',
                              activation=False)
         .relu(name='block1_conv5_relu')
         .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block1_conv5_conv1',
               activation=False)
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_block1_conv5_conv2',
                              activation=False)
         .relu(name=('block1_conv5_relu1'))
         .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block1_conv5_conv2', activation=False))
        (self.feed('block1_conv5_conv2', 'block1_conv4')
         .concat(axis=3, name='block1_conv5'))
        num_filters += growth_rate
        num_filters = int(compresson * num_filters)
        (self.feed('block1_conv5')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_tblock1_conv1',
                              activation=False)
         .relu(name=('tblock1_relu1'))
         .conv([1, 1], num_filters, [2, 2], biased=False, relu=False, name='tblock1_conv1', activation=False)
         .global_average_pooling('tblock1_pool'))
        for i in range(12):
            if i == 0:
                (self.feed('tblock1_pool')
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block2_conv%s_conv1' % str(i),
                                      activation=False)
                 .relu(name='block2_conv%s_relu' % str(i))
                 .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block2_conv%s_conv1' % str(i),
                       activation=False)
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block2_conv%s_conv2' % str(i),
                                      activation=False)
                 .relu(name=('block2_conv%s_relu1' % str(i)))
                 .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block2_conv%s_conv2' % str(i),
                       activation=False))
                (self.feed('block2_conv%s_conv2' % str(i), 'tblock1_pool')
                 .concat(axis=3, name='block2_conv%s' % str(i)))
                num_filters += growth_rate
            else:
                (self.feed('block2_conv%s' % str(i - 1))
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block2_conv%s_conv1' % str(i),
                                      activation=False)
                 .relu(name='block2_conv%s_relu' % str(i))
                 .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block2_conv%s_conv1' % str(i),
                       activation=False)
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block2_conv%s_conv2' % str(i),
                                      activation=False)
                 .relu(name=('block2_conv%s_relu1' % str(i)))
                 .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block2_conv%s_conv2' % str(i),
                       activation=False))
                (self.feed('block2_conv%s_conv2' % str(i), 'block2_conv%s' % str(i - 1))
                 .concat(axis=3, name='block2_conv%s' % str(i)))
                num_filters += growth_rate
        num_filters = int(compresson * num_filters)
        (self.feed('block2_conv11')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_tblock2_conv1',
                              activation=False)
         .relu(name=('tblock2_relu1'))
         .conv([1, 1], num_filters, [2, 2], biased=False, relu=False, name='tblock2_conv1', activation=False)
         .global_average_pooling('tblock2_pool'))

        for i in range(36):
            if i == 0:
                (self.feed('tblock2_pool')
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block3_conv%s_conv1' % str(i),
                                      activation=False)
                 .relu(name='block3_conv%s_relu' % str(i))
                 .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block3_conv%s_conv1' % str(i),
                       activation=False)
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block3_conv%s_conv2' % str(i),
                                      activation=False)
                 .relu(name=('block3_conv%s_relu1' % str(i)))
                 .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block3_conv%s_conv2' % str(i),
                       activation=False))
                (self.feed('block3_conv%s_conv2' % str(i), 'tblock3_pool')
                 .concat(axis=3, name='block3_conv%s' % str(i)))
                num_filters += growth_rate
            else:
                (self.feed('block3_conv%s' % str(i - 1))
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block3_conv%s_conv1' % str(i),
                                      activation=False)
                 .relu(name='block3_conv%s_relu' % str(i))
                 .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block3_conv%s_conv1' % str(i),
                       activation=False)
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block3_conv%s_conv2' % str(i),
                                      activation=False)
                 .relu(name=('block3_conv%s_relu1' % str(i)))
                 .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block3_conv%s_conv2' % str(i),
                       activation=False))
                (self.feed('block3_conv%s_conv2' % str(i), 'block3_conv%s' % str(i - 1))
                 .concat(axis=3, name='block3_conv%s' % str(i)))
                num_filters += growth_rate
        num_filters = int(compresson * num_filters)
        (self.feed('block3_conv35')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='bn_tblock3_conv1',
                              activation=False)
         .relu(name=('tblock3_relu1'))
         .conv([1, 1], num_filters, [2, 2], biased=False, relu=False, name='tblock3_conv1', activation=False)
         .global_average_pooling('tblock3_pool'))

        for i in range(16):
            if i == 0:
                (self.feed('tblock3_pool')
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block4_conv%s_conv1' % str(i),
                                      activation=False)
                 .relu(name='block4_conv%s_relu' % str(i))
                 .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block4_conv%s_conv1' % str(i),
                       activation=False)
                 # _conv
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block4_conv%s_conv2' % str(i),
                                      activation=False)
                 .relu(name=('block4_conv%s_relu1' % str(i)))
                 .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block4_conv%s_conv2' % str(i),
                       activation=False))
                (self.feed('block4_conv%s_conv2' % str(i), 'tblock3_pool')
                 .concat(axis=3, name='block4_conv%s' % str(i)))
                num_filters += growth_rate
            else:
                (self.feed('block4_conv%s' % str(i - 1))
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block4_conv%s_conv1' % str(i),
                                      activation=False)
                 .relu(name='block4_conv%s_relu' % str(i))
                 .conv([1, 1], num_filters * 4, [2, 2], biased=False, relu=False, name='block4_conv%s_conv1' % str(i),
                       activation=False)
                 .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu,
                                      name='bn_block4_conv%s_conv2' % str(i),
                                      activation=False)
                 .relu(name=('block4_conv%s_relu1' % str(i)))
                 .conv([3, 3], num_filters, [2, 2], biased=False, relu=False, name='block4_conv%s_conv2' % str(i),
                       activation=False))
                (self.feed('block4_conv%s_conv2' % str(i), 'block4_conv%s' % str(i - 1))
                 .concat(axis=3, name='block4_conv%s' % str(i)))
                num_filters += growth_rate

        (self.feed('block4_conv15')
         .batch_normalization(is_training=is_training, activation_fn=tf.nn.relu, name='final_bn', activation=False)
         .relu(name='final_relu')
         .atrous_conv([3, 3], num_classes, 6, padding='SAME', relu=False, name='fc1_voc12_c0')
         .global_average_pooling(name='final_pool'))

        (self.feed('final_relu')
         .atrous_conv([3, 3], num_classes, 12, padding='SAME', relu=False, name='fc1_voc12_c1'))

        (self.feed('final_relu')
         .atrous_conv([3, 3], num_classes, 18, padding='SAME', relu=False, name='fc1_voc12_c2'))

        (self.feed('final_relu')
         .atrous_conv([3, 3], num_classes, 24, padding='SAME', relu=False, name='fc1_voc12_c3'))

        (self.feed('fc1_voc12_c0',
                   'fc1_voc12_c1',
                   'fc1_voc12_c2',
                   'fc1_voc12_c3')
         .add(name='fc1_voc12'))

        def topredict(self, raw_output, origin_shape=None):
            raw_output = tf.image.resize_bilinear(raw_output, origin_shape)
            raw_output = tf.argmax(raw_output, axis=3)
            prediction = tf.expand_dims(raw_output, dim=3)
            return prediction
